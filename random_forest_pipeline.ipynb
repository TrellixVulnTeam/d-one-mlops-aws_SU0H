{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1> D ONE MLOps AWS BlogPost </h1>\n",
    "    <h2>Work In Progress</h2>\n",
    "<hr>\n",
    "<h1>Random Forest Trainig Pipeline</h1>\n",
    "<hr>\n",
    " </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Introduction</h2>\n",
    "\n",
    "* See EDA notebook\n",
    "* Add a few high level lines here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up shop\n",
    "\n",
    "A known challenge with SageMaker Studio default libraries is that they are often outdated. In this case, we want to make sure that pandas and s3fs is up-to-date. s3fs is a file system interface for S3 (https://pypi.org/project/s3fs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (2022.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: aiohttp<=4 in /opt/conda/lib/python3.7/site-packages (from s3fs) (3.8.1)\n",
      "Requirement already satisfied: aiobotocore~=2.2.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (2.2.0)\n",
      "Requirement already satisfied: fsspec==2022.3.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (2022.3.0)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.2.0->s3fs) (1.11.2)\n",
      "Requirement already satisfied: botocore<1.24.22,>=1.24.21 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.2.0->s3fs) (1.24.21)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore~=2.2.0->s3fs) (0.10.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (6.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (20.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp<=4->s3fs) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.14.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.22,>=1.24.21->aiobotocore~=2.2.0->s3fs) (1.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.22,>=1.24.21->aiobotocore~=2.2.0->s3fs) (1.26.9)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp<=4->s3fs) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install pandas s3fs --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "We need a few packages, let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import boto3  # Amazon Web Services (AWS) Software Development Kit (SDK) for Python\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::451811961115:role/AmazonSageMaker-ExecutionRole-20200526T075798'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the role that SageMaker assumes to leverage AWS resources\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the containers that we will create\n",
    "# S3 Bucket where the data is stored\n",
    "BUCKET_NAME = \"aws-sagemaker-blogpost\"\n",
    "BUCKET = f's3://{BUCKET_NAME}'\n",
    "\n",
    "# Raw data paths\n",
    "RAW_DATA_FOLDER = 'data'\n",
    "# Note if you change the line below, you also need to change it in the cell that creates the prepare_data.py\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "RAW_DATA_PATH = os.path.join(BUCKET, RAW_DATA_FOLDER, RAW_DATA_FILE)\n",
    "\n",
    "# Path where the processed objects will be stored\n",
    "now = datetime.now() # get current time to ensure uniqueness of the output folders\n",
    "PROCESSED_DATA_FOLDER = 'processed_' + now.strftime(\"%Y-%m-%d_%H%M_%S%f\")\n",
    "PROCESSED_DATA_PATH = os.path.join(BUCKET, PROCESSED_DATA_FOLDER)\n",
    "\n",
    "# Paths for model train, validation, test split\n",
    "TRAIN_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'train.csv')\n",
    "VALIDATION_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'validation.csv')\n",
    "TEST_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'test.csv')\n",
    "\n",
    "# Path to model output data\n",
    "MODEL_OUTPUT = os.path.join(BUCKET, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job name for preprocessing\n",
    "PREPROCESSING_JOB_NAME = 'windTurbinesPreprocessing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data (TODO)\n",
    "\n",
    "The data comes in chunks, but for our purposes, we would like to have one large file with data. We could also extend the code to read only new data and add it to the file, but for simplicity, let's load the whole dataset for processing.\n",
    "\n",
    "Note: Having the data in chunks would allow for online processing of new data when it comes in, e.g., when a turbine reports the next couple of months data.\n",
    "\n",
    "For details on the dataset, see the exploratory data analysis work in Part 1.\n",
    "\n",
    "* TODO: Expand\n",
    "* TODO: Link to EDA.\n",
    "\n",
    "We first create a preprocessing script that will be used by a built-in SKLearn container to run the preprocessing job of our pipeline. Running the cell below will produce a Python file whose content is the same as the cell's content and named `prepare_data.py`. It will be saved in the notebooks instance path (see left pane of the SageMaker Studio JupyterLab).\n",
    "\n",
    "We then define the container that will read in the data from the S3 bucket and use this script to preprocess it and then write the processed files back to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prepare_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_data.py\n",
    "\n",
    "import argparse\n",
    "import logger\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Union\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "def assert_col_of_df(df: pd.DataFrame, col: Union[List[str], str]) -> None:\n",
    "    \"\"\"Helper function to assert that a column `col` is a column of `df`.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe.\n",
    "        col: String value to test.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError if `col` is not a column of `df`.\n",
    "    \"\"\"\n",
    "    if isinstance(col, str):\n",
    "        col = [col]\n",
    "\n",
    "    for c in col:\n",
    "        try:\n",
    "            assert c in df.columns      \n",
    "        except AssertionError:\n",
    "            raise ValueError(f\"Invalid input value. Column {c} is not a column of df.\")\n",
    "\n",
    "            \n",
    "def get_train_test_split(\n",
    "        df: pd.DataFrame,\n",
    "        n_days_test: int\n",
    "        ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits the input data frame into a training and test set.\n",
    "\n",
    "    Args:\n",
    "        df: Raw input data.\n",
    "        n_days_test: Number of days to consider for the test split. The n_days_test last \n",
    "            days of the input data will be selected for the test split.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: Raw train and test data splits.\n",
    "    \"\"\"\n",
    "    _date_col = 'date'\n",
    "    _measured_at_col = 'measured_at'\n",
    "    \n",
    "    assert_col_of_df(df=df, col=_measured_at_col)\n",
    "    \n",
    "    # Take only the date part of the string, i.e., the first 10 characters\n",
    "    df[_date_col] = df[_measured_at_col].apply(lambda x: x[:10])\n",
    "    # Convert to date object\n",
    "    df[_date_col] = pd.to_datetime(df[_date_col], format='%Y-%m-%d')\n",
    "    \n",
    "    # Get the test dates\n",
    "    min_date = df[_date_col].min()\n",
    "    max_date = df[_date_col].max()\n",
    "    \n",
    "    test_dates = [\n",
    "        datetime.strftime(max_date - timedelta(days=i), '%Y-%m-%d') for i in range(n_days_test)\n",
    "    ]\n",
    "    \n",
    "    df_train = df[~df[_date_col].isin(test_dates)].drop(_date_col, axis=1)\n",
    "    df_test = df[df[_date_col].isin(test_dates)].drop(_date_col, axis=1)\n",
    "    \n",
    "    logger.info(f\"Train set ranges from {min_date} until {min(test_dates)} (not included).\")\n",
    "    logger.info(f\"Test set ranges from {min(test_dates)} until {max(test_dates)}.\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def fill_nulls(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Fills nulls in column `col` of dataframe `df`.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col: Column of `df` with nulls filled with 0.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with nulls filled.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col)\n",
    "        \n",
    "    df.loc[:, col] = (\n",
    "        df\n",
    "        .loc[:, col]\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Filled nulls in column {col} with 0.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_power(df: pd.DataFrame, col_power: str, min_power: float) -> pd.DataFrame:\n",
    "    \"\"\"Filters the `df` on the power column `col_power`.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col_power: Column of `df` with the power production.\n",
    "        min_power: Minimum values of power production considered. Rows with smaller\n",
    "            values are filtered out.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe filtered on `min_power`.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col_power)\n",
    "    \n",
    "    filter_power = df[col_power] > min_power\n",
    "    \n",
    "    rowcount_before = df.shape[0]\n",
    "    df = (\n",
    "        df\n",
    "        .loc[filter_power]\n",
    "    )\n",
    "    rowcount_after = df.shape[0]\n",
    "    logger.info(f\"Removed {rowcount_before-rowcount_after} rows which had power below {min_power}.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_error_types(\n",
    "    df: pd.DataFrame,\n",
    "    col_errors: str,\n",
    "    errors_to_classify: List[int]\n",
    "    )-> pd.DataFrame:\n",
    "    \"\"\"Transforms error types.\n",
    "    \n",
    "    Error types are classified according to the `errors_to_classify`. If the error\n",
    "    type is not in the list, it will be replaced with 9. Nulls are replaced with\n",
    "    0.\n",
    "\n",
    "    Args:\n",
    "        df: Raw input data.\n",
    "        col_errors: Column of `df` with the error types.\n",
    "        errors_to_classify: List of error types that should be considered.\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe with transformed error types.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col_errors)\n",
    "\n",
    "    df[col_errors] = (\n",
    "        df\n",
    "        .loc[:, col_errors]\n",
    "        .fillna(0)\n",
    "        .apply(lambda x: int(x))\n",
    "        .apply(lambda x: x if x in errors_to_classify else 9)\n",
    "    )\n",
    "    logger.info(f\"Transformed error types in column {col_errors}.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_features_target(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target: str\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Function to split the dataframe into input features and labels.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input data.\n",
    "        features: List of the features to be included in the transformation.\n",
    "        target: Target column.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: Transformed dataframes (input features and labels).\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=(features + [target]))\n",
    "\n",
    "    y = df.loc[:, target]\n",
    "    x = df.loc[:, features]\n",
    "    logger.info(f\"Split dataframe into features with shape {x.shape} and target with shape {y.shape}.\")\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def wrap_transform_data(\n",
    "    df: pd.DataFrame,\n",
    "    col_power: str,\n",
    "    min_power: float,\n",
    "    col_errors: str,\n",
    "    errors_to_classify: List[int],\n",
    "    features: List[str],\n",
    "    target: str\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Wrapper for transforming the data for the model\n",
    "    \n",
    "    Processing is applied in the following steps:\n",
    "        1. Filtering out low power values\n",
    "        2. Transforming error types\n",
    "        3. Fill nuls in all of the feature columns\n",
    "        4. Split dataset into features and target (x, y)\n",
    "        \n",
    "    Args:\n",
    "        df: Input dataframe.\n",
    "        col_power: Column of `df` with the power production.\n",
    "        min_power: Minimum values of power production considered. Rows with smaller\n",
    "            values are filtered out.\n",
    "        col_errors: Column of `df` with the error types.\n",
    "        errors_to_classify: List of error types that should be considered.\n",
    "        features: List of the features to be included in the transformation.\n",
    "        target: Target column.\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # 1. Filter out low power\n",
    "    df = filter_power(df=df, col_power=col_power, min_power=min_power)\n",
    "\n",
    "    # 2. Transform error types\n",
    "    df = transform_error_types(df=df, col_errors=col_errors, errors_to_classify=errors_to_classify)\n",
    "\n",
    "    # 3. Fill nulls in all of the feature columns\n",
    "    for feat in features:\n",
    "        df = fill_nulls(df=df, col=feat)\n",
    "\n",
    "    # 4. Split dataset into features and target\n",
    "    x, y = split_features_target(df=df, features=features, target=target) \n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# ----- CONSTANTS ----- #\n",
    "# Columns of df\n",
    "# Error column <> target\n",
    "COL_ERRORS = 'categories_sk'\n",
    "# Power produced column (used for filtering out small values)\n",
    "COL_POWER = 'power'\n",
    "# Error (i.e., target) classification list\n",
    "ERRORS_TO_CLASSIFY = [0, 3, 5, 7, 8]\n",
    "# Features to consider for the model\n",
    "FEATURES = ['wind_speed', 'power', 'nacelle_direction', 'wind_direction',\n",
    "            'rotor_speed', 'generator_speed', 'temp_environment',\n",
    "            'temp_hydraulic_oil', 'temp_gear_bearing', 'cosphi',\n",
    "            'blade_angle_avg', 'hydraulic_pressure']\n",
    "# Power values to filter out\n",
    "MIN_POWER = 0.05\n",
    "# Filname of the raw data file\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    logger.debug(f'Preprocessing job started.')\n",
    "    # Parse the SDK arguments that are passed when creating the SKlearn container\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_test_days\", type=int, default=10)\n",
    "    parser.add_argument(\"--n_val_days\", type=int, default=10)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.debug(f\"Received arguments {args}.\")\n",
    "\n",
    "    # Read in data locally in the container\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", RAW_DATA_FILE)\n",
    "    logger.debug(f\"Reading input data from {input_data_path}\")\n",
    "    # Read raw input data\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    logger.debug(f\"Shape of data is:{df.shape}\")\n",
    "\n",
    "    # ---- Preprocess the data set ----\n",
    "    logger.debug(\"Split data into training+validation and test set.\")\n",
    "    df_train_valid, df_test = get_train_test_split(df=df, n_days_test=args.n_test_days) \n",
    "\n",
    "    logger.debug(\"Split training+validation into training and validation set.\")\n",
    "    df_train, df_val = get_train_test_split(df=df, n_days_test=args.n_val_days) \n",
    "\n",
    "    logger.debug(\"Transforming training data.\")\n",
    "    x_train, y_train = wrap_transform_data(\n",
    "        df=df_train,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        col_errors=COL_ERRORS,\n",
    "        errors_to_classify=ERRORS_TO_CLASSIFY,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "    \n",
    "    logger.debug(\"Transforming validation data.\")\n",
    "    x_val, y_val = wrap_transform_data(\n",
    "        df=df_val,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        col_errors=COL_ERRORS,\n",
    "        errors_to_classify=ERRORS_TO_CLASSIFY,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "\n",
    "    # Create local output directories. These directories live on the container that is spun up.\n",
    "    try:\n",
    "        os.makedirs(\"/opt/ml/processing/train\")\n",
    "        os.makedirs(\"/opt/ml/processing/validation\")\n",
    "        os.makedirs(\"/opt/ml/processing/test\")\n",
    "        print(\"Successfully created directories\")\n",
    "    except Exception as e:\n",
    "        # if the Processing call already creates these directories (or directory otherwise cannot be created)\n",
    "        logger.debug(e)\n",
    "        logger.debug(\"Could Not Make Directories.\")\n",
    "        pass\n",
    "\n",
    "    # Save data locally on the container that is spun up.\n",
    "    try:\n",
    "        pd.DataFrame(x_train).to_csv(\"/opt/ml/processing/train/x_train.csv\", header=True, index=False)\n",
    "        pd.DataFrame(y_train).to_csv(\"/opt/ml/processing/train/y_train.csv\", header=True, index=False)\n",
    "        pd.DataFrame(x_val).to_csv(\"/opt/ml/processing/validation/x_val.csv\", header=True, index=False)\n",
    "        pd.DataFrame(y_val).to_csv(\"/opt/ml/processing/validation/y_val.csv\", header=True, index=False)\n",
    "        pd.DataFrame(df_test).to_csv(\"/opt/ml/processing/test/test.csv\", header=True, index=False)\n",
    "        logger.debug(\"Files Successfully Written Locally\")\n",
    "    except Exception as e:\n",
    "        logger.debug(\"Could Not Write the Files\")\n",
    "        logger.debug(e)\n",
    "        pass\n",
    "\n",
    "    logger.debug(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you read the script carefully you see that the training, validation, and test datasets are stored in the file path `/opt/ml/processing/...`. This is not the path in your S3 bucket, but refers to the local path on the container that is spun up. Do you know where you can find the train, validation, and test set? \n",
    "\n",
    "We specified that in the beginning when setting up path variables (e.g., `TEST_DATA_PATH`)! This will become useful later.\n",
    "The first line of the cell's output will indicate you the job's name. Go back to the AWS SageMaker Console and check the processing jobs. In my case - eu-central-1 AZ - the link is https://eu-central-1.console.aws.amazon.com/sagemaker/home?region=eu-central-1#/processing-jobs.\n",
    "\n",
    "From there, you can leverage CloudWatch to check the logs for the job or other monitoring tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  windTurbinesPreprocessing-2022-05-11-16-12-05-699\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aws-sagemaker-blogpost/data/wind_turbines.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-451811961115/windTurbinesPreprocessing-2022-05-11-16-12-05-699/input/code/prepare_data.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aws-sagemaker-blogpost/processed_2022-05-11_1604_19431566', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-2', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aws-sagemaker-blogpost/processed_2022-05-11_1604_19431566', 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aws-sagemaker-blogpost/processed_2022-05-11_1604_19431566', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".........."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_job_name = PREPROCESSING_JOB_NAME\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=base_job_name,\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=ROLE,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"prepare_data.py\",\n",
    "    inputs=[ProcessingInput(source=RAW_DATA_PATH, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    arguments=[\"--n_test_days\", \"20\",\n",
    "              \"--n_val_days\", \"30\"],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import (\n",
    "    get_train_test_split,\n",
    "    wrap_transform_data\n",
    ")\n",
    "# ----- CONSTANTS ----- #\n",
    "# Columns of df\n",
    "# Error column <> target\n",
    "COL_ERRORS = 'categories_sk'\n",
    "# Power produced column (used for filtering out small values)\n",
    "COL_POWER = 'power'\n",
    "# Error (i.e., target) classification list\n",
    "ERRORS_TO_CLASSIFY = [0, 3, 5, 7, 8]\n",
    "# Features to consider for the model\n",
    "FEATURES = ['wind_speed', 'power', 'nacelle_direction', 'wind_direction',\n",
    "            'rotor_speed', 'generator_speed', 'temp_environment',\n",
    "            'temp_hydraulic_oil', 'temp_gear_bearing', 'cosphi',\n",
    "            'blade_angle_avg', 'hydraulic_pressure']\n",
    "# Power values to filter out\n",
    "MIN_POWER = 0.05\n",
    "# Number of days in the (hidden) test set\n",
    "N_TEST_DAYS = 20\n",
    "# Number of days in the validation set\n",
    "N_VAL_DAYS = 20\n",
    "\n",
    "\n",
    "# Read raw input data\n",
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "\n",
    "# Split data into training+validation and test set\n",
    "df_train_valid, df_test = get_train_test_split(df=df, n_days_test=N_TEST_DAYS) \n",
    "\n",
    "# Split training+validation into training and validation set\n",
    "df_train, df_val = get_train_test_split(df=df, n_days_test=N_VAL_DAYS) \n",
    "\n",
    "x_train, y_test = wrap_transform_data(\n",
    "    df=df_train,\n",
    "    col_power=COL_POWER,\n",
    "    min_power=MIN_POWER,\n",
    "    col_errors=COL_ERRORS,\n",
    "    errors_to_classify=ERRORS_TO_CLASSIFY,\n",
    "    features=FEATURES,\n",
    "    target=COL_ERRORS\n",
    ")\n",
    "\n",
    "x_val, y_val = wrap_transform_data(\n",
    "    df=df_val,\n",
    "    col_power=COL_POWER,\n",
    "    min_power=MIN_POWER,\n",
    "    col_errors=COL_ERRORS,\n",
    "    errors_to_classify=ERRORS_TO_CLASSIFY,\n",
    "    features=FEATURES,\n",
    "    target=COL_ERRORS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020-03-12 00:00:00.0000000 +01:00', '2020-03-31 17:00:00.0000000 +01:00')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val['measured_at'].min(), x_val['measured_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2020-01-01 01:00:00.0000000 +01:00', '2020-03-11 23:50:00.0000000 +01:00')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['measured_at'].min(), x_train['measured_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = df['date'].min()\n",
    "max_date = df['date'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dates = [datetime.strftime(max_date - timedelta(days=i), '%Y-%m-%d') for i in range(3)]\n",
    "test_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_and_transform_data(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
