{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Machine Learning in Production (MLOps) with AWS</h1>\n",
    "<hr>\n",
    "<h1>Using AWS SageMaker to Build an XGBoost Model and to Deploy it to an Endpoint</h1>\n",
    "<hr>\n",
    "<h5>https://www.d-one.ai, July 2022</h5>\n",
    "<h6>Heiko Kromer (Heiko.Kromer@d-one.ai)</h6>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Introduction</h2>\n",
    "\n",
    "<div class=\"alert alert-info\">ðŸš¨<strong>Add link to EDA notebook</strong></div>\n",
    "\n",
    "In this notebook, we will set up and end-to-end process to preprocess, train, and deploy an XGBoost model to an endpoint in AWS SageMaker. The dataset we use is taken from WinJi (https://www.win-ji.com), which is an asset management platform for renewables. The dataset contains various physical measurements as per the column definition below:\n",
    "\n",
    "| Column name        | Description                                                                           | Unit | Data Type |\n",
    "|:-------------------|:--------------------------------------------------------------------------------------|:-----|:----------|\n",
    "| wt_sk              | Unique device identifier, equivalent to device name                             | -    | float     |\n",
    "| measured_at        | Data timestemp in UTC format                                                          | -    | string    |\n",
    "| wind_speed         | Average apparent wind speed measured by nacelle anemometer, normalised to rated value | m/s  | float     |\n",
    "| power              | Average measured power production, normalised to rated max power                      | W    | float     |\n",
    "| nacelle_direction  | Average position of nacelle relative to North (E=90Â°)                                 | Â°    | float     |\n",
    "| wind_direction     | Average direction of incoming wind relative to North (E=90Â°)                          | Â°    | float     |\n",
    "| rotor_speed        | Average revolutions per minute of the low speed rotor, normalised to rates RS         | -    | float     |\n",
    "| generator_speed    | Average revolutions per minute of the generator, normalised to rated GS               | -    | float     |\n",
    "| temp_environment   | Average outside temperature on nacelle height                                         | Â°C   | float     |\n",
    "| temp_hydraulic_oil | Average oil temperature                                                               | Â°C   | float     |\n",
    "| temp_gear_bearing  | Average gear temperature                                                              | Â°C   | float     |\n",
    "| cosphi             | Average power factor of device                                                        | -    | float     |\n",
    "| blade_angle_avg    | Average pitching angles, averaged over blades                                         | Â°    | float     |\n",
    "| hydraulic_pressure | Average pressure in hydraulic circuit                                                 | mBar | float     |\n",
    "| subtraction        | Error flag (NaN: no error, 0/1: error)                                                | -    | float     |\n",
    "| categories_sk      | Categorisation of error type                                                          | -    | float     |\n",
    "\n",
    "Values have been obfuscated and white noise added to prevent leakage of proprietary information. For details on the exploration of the dataset, see the data exploration notebook.\n",
    "\n",
    "The goal in this notebook is to use AWS SageMaker to predict an error state for a turbine (column `subtraction`) using an XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up shop\n",
    "\n",
    "A known challenge with SageMaker Studio default libraries is that they are often outdated. In this case, we want to make sure that pandas and s3fs is up-to-date. s3fs is a file system interface for S3 (https://pypi.org/project/s3fs/).\n",
    "\n",
    "We also want to install XGBoost to visualize the feature importances of the model. We will not use this installed XGBoost package, but run the model in a container preconfigure by AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (2022.3.0)\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-2022.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: aiohttp<=4 in /opt/conda/lib/python3.7/site-packages (from s3fs) (3.8.1)\n",
      "Collecting fsspec==2022.5.0\n",
      "  Using cached fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "Collecting aiobotocore~=2.3.0\n",
      "  Using cached aiobotocore-2.3.3.tar.gz (65 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31mâ•°â”€>\u001b[0m \u001b[31m[20 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 36, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-otsbypmf/aiobotocore_ef0a856a92f64a0bbdf2a39abb80686b/setup.py\", line 70, in <module>\n",
      "  \u001b[31m   \u001b[0m     include_package_data=True\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/core.py\", line 109, in setup\n",
      "  \u001b[31m   \u001b[0m     _setup_distribution = dist = klass(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 466, in __init__\n",
      "  \u001b[31m   \u001b[0m     for k, v in attrs.items()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 293, in __init__\n",
      "  \u001b[31m   \u001b[0m     self.finalize_options()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 885, in finalize_options\n",
      "  \u001b[31m   \u001b[0m     for ep in sorted(loaded, key=by_order):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 884, in <lambda>\n",
      "  \u001b[31m   \u001b[0m     loaded = map(lambda e: e.load(), filtered)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_vendor/importlib_metadata/__init__.py\", line 196, in load\n",
      "  \u001b[31m   \u001b[0m     return functools.reduce(getattr, attrs, module)\n",
      "  \u001b[31m   \u001b[0m AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: xgboost==1.6.1 in /opt/conda/lib/python3.7/site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.6.1) (1.4.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from xgboost==1.6.1) (1.21.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install pandas s3fs --upgrade\n",
    "%pip install xgboost==1.6.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few packages, let's import them. Notably is the `boto3` package, which is AWS's SDK for Python. It is named after a dolphin that is swimming in the Amazonas river (https://en.wikipedia.org/wiki/Amazon_river_dolphin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "from datetime import datetime, timedelta\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List\n",
    "\n",
    "import boto3\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import sagemaker\n",
    "import tarfile\n",
    "import xgboost\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up global variables\n",
    "\n",
    "We will set a up a few variables to make our lifes easier.\n",
    "\n",
    "Let us first check on the role that resources created from our code will assume to leverage other AWS resources via API calls. In case we need additional permissions, we can edit the role policy in IAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::451811961115:role/AmazonSageMaker-ExecutionRole-20200526T075798'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the role that SageMaker assumes to leverage AWS resources\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "ROLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are our definitions for the paths. We want the artifacts created from the compute instances to be copied into an S3 bucket. The processed data will be placed into a folder that is prefixed with the current time to ensure uniqueness of the output folder. \n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸŽ¯ <strong> Edit S3 bucket name </strong>\n",
    "    \n",
    "Replace the bucket name with the name of your bucket. Note that the permissions of the bucket must allow access to the role. In my case, I have given the role full access to the S3 bucket named \"aws-sagemaker-blogpost\".\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the containers that we will create\n",
    "# S3 Bucket where the data is stored\n",
    "BUCKET_NAME = \"sagemaker-done-mlops\"  # Put your S3 bucket name here\n",
    "BUCKET = f's3://{BUCKET_NAME}'\n",
    "\n",
    "# Raw data paths\n",
    "RAW_DATA_FOLDER = 'data'\n",
    "# Note if you change the line below, you also need to change it in the cell that creates the prepare_data.py\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "RAW_DATA_PATH = os.path.join(BUCKET, RAW_DATA_FOLDER, RAW_DATA_FILE)\n",
    "\n",
    "# Path where the processed objects will be stored\n",
    "now = datetime.now() # get current time to ensure uniqueness of the output folders\n",
    "PROCESSED_DATA_FOLDER = 'processed_' + now.strftime(\"%Y-%m-%d_%H%M_%S%f\")\n",
    "PROCESSED_DATA_PATH = os.path.join(BUCKET, PROCESSED_DATA_FOLDER)\n",
    "\n",
    "# Paths for model train, validation, test split\n",
    "TRAIN_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'train.csv')\n",
    "TRAIN_DATA_PATH_W_HEADER = os.path.join(PROCESSED_DATA_PATH, 'train_w_header.csv')\n",
    "VALIDATION_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'validation.csv')\n",
    "TEST_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'test.csv')\n",
    "TEST_DATA_PATH_W_HEADER = os.path.join(PROCESSED_DATA_PATH, 'test_w_header.csv')\n",
    "\n",
    "# Path to model output data\n",
    "MODEL_OUTPUT = os.path.join(BUCKET, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job name for preprocessing, you can pick any name\n",
    "PREPROCESSING_JOB_NAME= 'windTurbinesPreprocessing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "<div class=\"alert alert-info\">ðŸš¨<strong>Add link to EDA notebook</strong></div>\n",
    "\n",
    "The data comes in chunks, but for our purposes, we would like to have one large file with data. We could also extend the code to read only new data and add it to the file, but for simplicity, let's load the whole dataset for processing.\n",
    "\n",
    "Note: Having the data in chunks would allow for online processing of new data when it comes in, e.g., when a turbine reports the next couple of months data. We could set this up easily with EventBridge for example.\n",
    "\n",
    "For details on the dataset, see the exploratory data analysis notebook.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format. Here, we stick to CSV. The requirements on the data formatting are:\n",
    "\n",
    "1. The target variable must be in the first column.\n",
    "2. There cannot be a header row.\n",
    "\n",
    "We first create a preprocessing script that will be used by a built-in SKLearn container to run the preprocessing job of our pipeline. Running the cell below will produce a Python file whose content is the same as the cell's content and named `prepare_data.py`. It will be saved in the notebooks instance path (see left pane of the SageMaker Studio JupyterLab). We need to provide the constants in the cell (and redefine for example the dataset file name). Reason is that the cell below creates a file (prepare_data.py) which is provided to the preprocessing instance.\n",
    "\n",
    "We then define the container that will read in the data from the S3 bucket and use this script to preprocess it and then write the processed files back to the S3 bucket. \n",
    "\n",
    "## Preprocessing steps\n",
    "\n",
    "We are applying the following preprocessing steps:\n",
    "\n",
    "1. Filtering out low power values\n",
    "    - We set a threshold of 0.05 W. Measurements with power values below this threshold are not considered because the turbines might have been in a non working state.\n",
    "2. Process target column\n",
    "    - We replace the two error types 1 and 0 with 1.\n",
    "    - We fill nulls with 0.\n",
    "3. Fill nulls in all of the feature columns\n",
    "    - We select only relevant column of the dataset and fill nulls in the dataset with 0.\n",
    "4. Select only relevant columns\n",
    "    - We make sure the target column is in the first column of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prepare_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_data.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "# ----- CONSTANTS ----- #\n",
    "# Columns of df\n",
    "# Error column <> target\n",
    "COL_ERRORS = 'subtraction'\n",
    "# Power produced column (used for filtering out small values)\n",
    "COL_POWER = 'power'\n",
    "# Features to consider for the model\n",
    "FEATURES = ['wind_speed', 'power', 'nacelle_direction', 'wind_direction',\n",
    "            'rotor_speed', 'generator_speed', 'temp_environment',\n",
    "            'temp_hydraulic_oil', 'temp_gear_bearing', 'cosphi',\n",
    "            'blade_angle_avg', 'hydraulic_pressure']\n",
    "# Power values to filter out\n",
    "MIN_POWER = 0.05\n",
    "# Filname of the raw data file\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "\n",
    "\n",
    "def assert_col_of_df(df: pd.DataFrame, col: Union[List[str], str]) -> None:\n",
    "    \"\"\"Helper function to assert that a column `col` is a column of `df`.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe.\n",
    "        col: String value to test.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError if `col` is not a column of `df`.\n",
    "    \"\"\"\n",
    "    if isinstance(col, str):\n",
    "        col = [col]\n",
    "\n",
    "    for c in col:\n",
    "        try:\n",
    "            assert c in df.columns      \n",
    "        except AssertionError:\n",
    "            raise ValueError(f\"Invalid input value. Column {c} is not a column of df.\")\n",
    "\n",
    "            \n",
    "def get_train_test_split(\n",
    "        df: pd.DataFrame,\n",
    "        n_days_test: int\n",
    "        ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits the input data frame into a training and test set.\n",
    "\n",
    "    Args:\n",
    "        df: Raw input data.\n",
    "        n_days_test: Number of days to consider for the test split. The n_days_test last \n",
    "            days of the input data will be selected for the test split.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: Raw train and test data splits.\n",
    "    \"\"\"\n",
    "    _date_col = 'date'\n",
    "    _measured_at_col = 'measured_at'\n",
    "    \n",
    "    assert_col_of_df(df=df, col=_measured_at_col)\n",
    "    \n",
    "    # Take only the date part of the string, i.e., the first 10 characters\n",
    "    df[_date_col] = df[_measured_at_col].apply(lambda x: x[:10])\n",
    "    # Convert to date object\n",
    "    df[_date_col] = pd.to_datetime(df[_date_col], format='%Y-%m-%d')\n",
    "    \n",
    "    # Get the test dates\n",
    "    min_date = df[_date_col].min()\n",
    "    max_date = df[_date_col].max()\n",
    "    \n",
    "    test_dates = [\n",
    "        datetime.strftime(max_date - timedelta(days=i), '%Y-%m-%d') for i in range(n_days_test)\n",
    "    ]\n",
    "    \n",
    "    df_train = df[~df[_date_col].isin(test_dates)].drop(_date_col, axis=1)\n",
    "    df_test = df[df[_date_col].isin(test_dates)].drop(_date_col, axis=1)\n",
    "    \n",
    "    logger.info(f\"Train set ranges from {min_date}Â until {min(test_dates)} (not included).\")\n",
    "    logger.info(f\"Test set ranges from {min(test_dates)}Â until {max(test_dates)}.\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def fill_nulls(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Fills nulls in column `col` of dataframe `df`.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col: Column of `df` with nulls filled with 0.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with nulls filled.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col)\n",
    "        \n",
    "    df.loc[:, col] = (\n",
    "        df\n",
    "        .loc[:, col]\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Filled nulls in column {col} with 0.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_power(df: pd.DataFrame, col_power: str, min_power: float) -> pd.DataFrame:\n",
    "    \"\"\"Filters the `df` on the power column `col_power`.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col_power: Column of `df` with the power production.\n",
    "        min_power: Minimum values of power production considered. Rows with smaller\n",
    "            values are filtered out.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe filtered on `min_power`.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col_power)\n",
    "    \n",
    "    filter_power = df[col_power] > min_power\n",
    "    \n",
    "    rowcount_before = df.shape[0]\n",
    "    df = (\n",
    "        df\n",
    "        .loc[filter_power]\n",
    "    )\n",
    "    rowcount_after = df.shape[0]\n",
    "    logger.info(f\"Removed {rowcount_before-rowcount_after} rows which had power below {min_power}.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_target(df: pd.DataFrame, col_target: str) -> pd.DataFrame:\n",
    "    \"\"\"Processes the target column by:\n",
    "        1. Replacing error types 1 and 0 with 1.\n",
    "        2. Filling nulls with 0.\n",
    "        3. Make sure the target column is in the first column of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col_target: Target column\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with target column processed.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col_target)\n",
    "    \n",
    "    # Make sure that the 0 error type is also mapped to 1 (we do a binary classification later)\n",
    "    df.loc[df[col_target] == 0, col_target] = 1\n",
    "    \n",
    "    df = fill_nulls(df=df, col=col_target)\n",
    "    \n",
    "    # Reorder columns\n",
    "    colnames = list(df.columns)\n",
    "    colnames.insert(0, colnames.pop(colnames.index(col_target)))\n",
    "    df = df[colnames]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def wrap_transform_data(\n",
    "    df: pd.DataFrame,\n",
    "    col_power: str,\n",
    "    min_power: float,\n",
    "    features: List[str],\n",
    "    target: str\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Wrapper for transforming the data for the model\n",
    "    \n",
    "    Processing is applied in the following steps:\n",
    "        1. Filtering out low power values\n",
    "        2. Process target column\n",
    "        3. Fill nulls in all of the feature columns\n",
    "        4. Select only relevant columns\n",
    "        \n",
    "    Args:\n",
    "        df: Input dataframe.\n",
    "        col_power: Column of `df` with the power production.\n",
    "        min_power: Minimum values of power production considered. Rows with smaller\n",
    "            values are filtered out.\n",
    "        features: List of the features to be included in the transformation.\n",
    "        target: Target column.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed dataframe.\n",
    "    \"\"\"\n",
    "    # 1. Filter out low power\n",
    "    df = filter_power(df=df, col_power=col_power, min_power=min_power)\n",
    "    \n",
    "    # 2. Process target clumn\n",
    "    df = process_target(df=df, col_target=target)\n",
    "\n",
    "    # 3. Fill nulls in all of the feature columns and select them\n",
    "    for feat in features:\n",
    "        df = fill_nulls(df=df, col=feat)\n",
    "    \n",
    "    # 4. Select only relevant columns\n",
    "    df = df[[target] + features]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    logger.info(f'Preprocessing job started.')\n",
    "    # Parse the SDK arguments that are passed when creating the SKlearn container\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_test_days\", type=int, default=10)\n",
    "    parser.add_argument(\"--n_val_days\", type=int, default=10)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.info(f\"Received arguments {args}.\")\n",
    "\n",
    "    # Read in data locally in the container\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", RAW_DATA_FILE)\n",
    "    logger.info(f\"Reading input data from {input_data_path}\")\n",
    "    # Read raw input data\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    logger.info(f\"Shape of data is: {df.shape}\")\n",
    "\n",
    "    # ---- Preprocess the data set ----\n",
    "    logger.info(\"Split data into training+validation and test set.\")\n",
    "    df_train_valid, df_test = get_train_test_split(df=df, n_days_test=args.n_test_days) \n",
    "\n",
    "    logger.info(\"Split training+validation into training and validation set.\")\n",
    "    df_train, df_val = get_train_test_split(df=df_train_valid, n_days_test=args.n_val_days) \n",
    "\n",
    "    logger.info(\"Transforming training data.\")\n",
    "    train = wrap_transform_data(\n",
    "        df=df_train,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Transforming validation data.\")\n",
    "    val = wrap_transform_data(\n",
    "        df=df_val,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "\n",
    "    logger.info(\"Transforming test data.\")\n",
    "    test = wrap_transform_data(\n",
    "        df=df_test,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "    \n",
    "    # Create local output directories. These directories live on the container that is spun up.\n",
    "    try:\n",
    "        os.makedirs(\"/opt/ml/processing/train\")\n",
    "        os.makedirs(\"/opt/ml/processing/validation\")\n",
    "        os.makedirs(\"/opt/ml/processing/test\")\n",
    "        print(\"Successfully created directories\")\n",
    "    except Exception as e:\n",
    "        # if the Processing call already creates these directories (or directory otherwise cannot be created)\n",
    "        logger.debug(e)\n",
    "        logger.debug(\"Could Not Make Directories.\")\n",
    "        pass\n",
    "\n",
    "    # Save data locally on the container that is spun up.\n",
    "    try:\n",
    "        pd.DataFrame(train).to_csv(\"/opt/ml/processing/train/train.csv\", header=False, index=False)\n",
    "        pd.DataFrame(train).to_csv(\"/opt/ml/processing/train/train_w_header.csv\", header=True, index=False)\n",
    "        pd.DataFrame(val).to_csv(\"/opt/ml/processing/validation/val.csv\", header=False, index=False)\n",
    "        pd.DataFrame(val).to_csv(\"/opt/ml/processing/validation/val_w_header.csv\", header=True, index=False)\n",
    "        pd.DataFrame(test).to_csv(\"/opt/ml/processing/test/test.csv\", header=False, index=False)\n",
    "        pd.DataFrame(test).to_csv(\"/opt/ml/processing/test/test_w_header.csv\", header=True, index=False)\n",
    "        logger.info(\"Files Successfully Written Locally\")\n",
    "    except Exception as e:\n",
    "        logger.debug(\"Could Not Write the Files\")\n",
    "        logger.debug(e)\n",
    "        pass\n",
    "\n",
    "    logger.info(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one reads the script carefully one can see that the training, validation, and test datasets are stored in the file path `/opt/ml/processing/...`. This is not the path in the S3 bucket, but refers to the local path on the container that is spun up. We supply the input data below when running the processor (see argument `inputs`).\n",
    "\n",
    "We specified that in the beginning when setting up path variables (e.g., `TEST_DATA_PATH`)! This will become useful later.\n",
    "The first line of the cell's output will indicate the job's name. Go back to the AWS SageMaker Console and check the processing jobs. In my case - eu-central-1 AZ - the link is https://eu-central-1.console.aws.amazon.com/sagemaker/home?region=eu-central-1#/processing-jobs.\n",
    "\n",
    "From there, one can leverage CloudWatch to check the logs for the job or make use of other monitoring tools - the full power of AWS can be leveraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  windTurbinesPreprocessing-2022-06-20-11-23-06-156\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-done-mlops/data/wind_turbines.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-451811961115/windTurbinesPreprocessing-2022-06-20-11-23-06-156/input/code/prepare_data.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-done-mlops/processed_2022-06-20_1123_05856437', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-2', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-done-mlops/processed_2022-06-20_1123_05856437', 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-done-mlops/processed_2022-06-20_1123_05856437', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      "."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_job_name = PREPROCESSING_JOB_NAME\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=base_job_name,\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=ROLE,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"prepare_data.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=RAW_DATA_PATH,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/validation\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/test\"\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--n_test_days\", \"20\",\n",
    "        \"--n_val_days\", \"30\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "For this classification task, we use a bult-in XGBoost container provided by SageMaker. With `sagemaker.image_uri` we specify the training container image URI. In our case, the SageMaker XGBoost training container URI is specified using `sagemaker.image_uris.retrieve`.\n",
    "\n",
    "You can find information on pre-built containers for example here\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-docker-containers-scikit-learn-spark.html or by exploring the SageMaker example notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region_name,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "print(f'Using SageMaker container: {container} in region {region_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the SageMaker Estimator with resource configurations and hyperparameters to train a logistic binary classifier on an c4.4xlarge instance. For different types of EC2 instances available from SageMaker, see https://aws.amazon.com/ec2/instance-explorer/?ec2-instances-cards.sort-by=item.additionalFields.category-order&ec2-instances-cards.sort-order=asc&awsf.ec2-instances-filter-category=*all&awsf.ec2-instances-filter-processors=*all&awsf.ec2-instances-filter-accelerators=*all&awsf.ec2-instances-filter-capabilities=*all\n",
    "\n",
    "Specifying the `rules` with `create_xgboost_report()` creates an XGBoost report that provides insights into the training progress and results (see more information here: https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-training-xgboost-report.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "xgboost_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=ROLE, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.4xlarge',\n",
    "    output_path=MODEL_OUTPUT,\n",
    "    sagemaker_session=session,\n",
    "    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n",
    ")\n",
    "\n",
    "# set the model's hyperparameters\n",
    "xgboost_model.set_hyperparameters(\n",
    "    objective='binary:logistic',\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "            s3_data=f'{PROCESSED_DATA_PATH}/train.csv',\n",
    "            distribution='FullyReplicated',\n",
    "            content_type=\"text/csv\",\n",
    "            s3_data_type='S3Prefix')\n",
    "\n",
    "validation_data = sagemaker.inputs.TrainingInput(\n",
    "                s3_data=f'{PROCESSED_DATA_PATH}/val.csv',\n",
    "                distribution='FullyReplicated',\n",
    "                content_type=\"text/csv\",\n",
    "                s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgboost_model.fit(inputs=data_channels, logs='All', wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_report_path = xgboost_model.output_path + '/' + xgboost_model.latest_training_job.job_name + \"/rule-output\"\n",
    "\n",
    "! aws s3 ls {xgboost_report_path} --recursive\n",
    "! aws s3 cp {xgboost_report_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Click link below to view the XGBoost Training report\", FileLink(\"CreateXgboostReport/xgboost_report.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "After training, we want to evaluate the model and in particular find out what importance the model gives to each of the features.\n",
    "\n",
    "For that, we first copy the model to the path where this notebook is running to import it. For this, we need to have `xgboost` imported locally as we did in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'{xgboost_model.output_path}/{xgboost_model.latest_training_job.job_name}/output/model.tar.gz'\n",
    "\n",
    "!aws s3 cp $model_path \"model.tar.gz\"\n",
    "\n",
    "#opens the downloaded model artifact and loads it as 'model' variable\n",
    "tar = tarfile.open('model.tar.gz')\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "model = pkl.load(open('xgboost-model', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the box, the model does not contain column headers, hence we do not know what feature is which, only the order is preserved. We can map the features from the earlier defined processed dataset with headers to the xgboost model features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DATA_PATH_W_HEADER)\n",
    "map_names = dict(zip(model.get_fscore().keys(), list(df.columns)[1:]))\n",
    "map_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_names = list(map_names.values())\n",
    "#plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "xgboost.plot_importance(model, importance_type='gain', max_num_features=30, height=0.8, ax=ax, show_values = False)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">ðŸš¨<strong>Discuss with Spyros/WinJi</strong></div>\n",
    "\n",
    "This result is somewhat surprising, as we see that the wind speed and blade angle have a high effect on the outcome of the model. We will stop this investigation here, we could go deeper with SHAP values and try to understand the models predictions. For the purpose of this notebook, we leave it here, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoking the endpoint\n",
    "\n",
    "With the code below, we invoke the endpoint to be called from inside AWS (or outside). In the AWS Console, we can investigate this endpoint and open it up to the public, if we wish. We chose a smaller instance for inference as compared to training, because we do not need as many resources for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint = xgboost_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the two functions defined below to call the endpoint. The first function converts a dataframe to a payload (csv) to be consumed by the model. The second function passes the payload to an endpoint and returns the predictions as a list of floats, i.e., probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_csv_object(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Converts the dataframe object `df` to a payload that can be passed to the model endpoint.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe that is converted to a csv-file for the SageMaker model's endpoint.\n",
    "    \n",
    "    Returns:\n",
    "        payload_as_csv: csv-file as payload.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert isinstance(df, pd.DataFrame)\n",
    "    \n",
    "    csv_file = io.StringIO()\n",
    "    # by default sagemaker expects comma separated\n",
    "    df.to_csv(csv_file, sep=\",\", header=False, index=False)\n",
    "    \n",
    "    payload_as_csv = csv_file.getvalue()\n",
    "    \n",
    "    return payload_as_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prediction(payload: str, endpoint: sagemaker.predictor.Predictor) -> List[float]:\n",
    "    \"\"\"Function to pass a single payload to the model endpoint and return a prediction.\n",
    "    \n",
    "    Args:\n",
    "        payload: SageMaker model friendly payload (feature vector).\n",
    "        endpoint: SageMaker model endpoint.\n",
    "    \n",
    "    Returns:\n",
    "        response: List of predictions from the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # invoke the endpoint to get a prediction\n",
    "    response = endpoint.predict(payload, initial_args={'ContentType': \"text/csv\"})\n",
    "    \n",
    "    # process predictions\n",
    "    response = response.decode(\"utf-8\")\n",
    "    \n",
    "    response = [float(i) for i in response.split(',')]\n",
    "    \n",
    "    # convert to array\n",
    "    response = np.asarray(response)\n",
    "      \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model (Example how the endpoint can be used)\n",
    "\n",
    "As an example how we can make use of the endpoint, we evaluate the model by checking metrics and the confusion matrix. We do this on both the training and the test data. \n",
    "\n",
    "We could also use a service such as postman to call the model or define our own Lambda functions to make calls to the model endpoint. For CI/CD processes, we would create a pipeline object that we can maintain and use for further development. All this can be done with Jupyter notebooks as well, but we will use the AWS SageMaker Studio in a next step. However, behind the scenes the studio does things very similar to what is going on in this notebook, as we will see in the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(true: np.ndarray, prediction: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Helper function to create a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        true: True labels.\n",
    "        prediction: Predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Confusion matrix.\n",
    "    \"\"\"\n",
    "    c_mat = confusion_matrix(true, prediction)\n",
    "    return c_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_DATA_PATH_W_HEADER)\n",
    "train_features = train_data.iloc[:, 1:]\n",
    "\n",
    "prediction = batch_prediction(\n",
    "    payload=convert_df_to_csv_object(df=train_features),\n",
    "    endpoint=model_endpoint\n",
    ")\n",
    "prediction_binary = np.where(prediction > THRESHOLD, 1, 0)\n",
    "\n",
    "cm = create_confusion_matrix(true=train_data.iloc[:, 0].values, prediction=prediction_binary)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = round(roc_auc_score(train_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "accuracy = round(accuracy_score(train_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "recall = round(recall_score(train_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "precision = round(precision_score(train_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "print(f'AUC is {auc}')\n",
    "print(f'Accuracy is {accuracy}')\n",
    "print(f'Recall is {recall}')\n",
    "print(f'Precision is {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(TEST_DATA_PATH_W_HEADER)\n",
    "test_features = test_data.iloc[:, 1:]\n",
    "\n",
    "prediction = batch_prediction(\n",
    "    payload=convert_df_to_csv_object(df=test_features),\n",
    "    endpoint=model_endpoint\n",
    ")\n",
    "prediction_binary = np.where(prediction > THRESHOLD, 1, 0)\n",
    "\n",
    "cm = create_confusion_matrix(true=test_data.iloc[:, 0].values, prediction=prediction_binary)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = round(roc_auc_score(test_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "accuracy = round(accuracy_score(test_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "recall = round(recall_score(test_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "precision = round(precision_score(test_data.iloc[:, 0].values, prediction_binary), 4)\n",
    "print(f'AUC is {auc}')\n",
    "print(f'Accuracy is {accuracy}')\n",
    "print(f'Recall is {recall}')\n",
    "print(f'Precision is {precision}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_endpoint.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
