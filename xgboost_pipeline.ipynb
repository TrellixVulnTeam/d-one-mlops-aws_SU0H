{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1> D ONE MLOps AWS BlogPost </h1>\n",
    "    <h2>Work In Progress</h2>\n",
    "<hr>\n",
    "<h1>XGBoost Trainig Pipeline</h1>\n",
    "<hr>\n",
    " </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Introduction</h2>\n",
    "\n",
    "* See EDA notebook\n",
    "* Add a few high level lines here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up shop\n",
    "\n",
    "A known challenge with SageMaker Studio default libraries is that they are often outdated. In this case, we want to make sure that pandas and s3fs is up-to-date. s3fs is a file system interface for S3 (https://pypi.org/project/s3fs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (2022.3.0)\n",
      "Collecting s3fs\n",
      "  Using cached s3fs-2022.5.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\n",
      "Collecting fsspec==2022.5.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiobotocore~=2.3.0\n",
      "  Using cached aiobotocore-2.3.2.tar.gz (104 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[20 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 36, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-pcd02ia7/aiobotocore_0c3fb1baad4f4429964591a639acc0e6/setup.py\", line 70, in <module>\n",
      "  \u001b[31m   \u001b[0m     include_package_data=True\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/__init__.py\", line 87, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/core.py\", line 109, in setup\n",
      "  \u001b[31m   \u001b[0m     _setup_distribution = dist = klass(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 466, in __init__\n",
      "  \u001b[31m   \u001b[0m     for k, v in attrs.items()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_distutils/dist.py\", line 293, in __init__\n",
      "  \u001b[31m   \u001b[0m     self.finalize_options()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 885, in finalize_options\n",
      "  \u001b[31m   \u001b[0m     for ep in sorted(loaded, key=by_order):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/dist.py\", line 884, in <lambda>\n",
      "  \u001b[31m   \u001b[0m     loaded = map(lambda e: e.load(), filtered)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.7/site-packages/setuptools/_vendor/importlib_metadata/__init__.py\", line 196, in load\n",
      "  \u001b[31m   \u001b[0m     return functools.reduce(getattr, attrs, module)\n",
      "  \u001b[31m   \u001b[0m AttributeError: type object 'Distribution' has no attribute '_finalize_feature_opts'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "! pip install pandas s3fs --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n",
    "We need a few packages, let's import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import boto3  # Amazon Web Services (AWS) Software Development Kit (SDK) for Python\n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::451811961115:role/AmazonSageMaker-ExecutionRole-20200526T075798'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the role that SageMaker assumes to leverage AWS resources\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "ROLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for the containers that we will create\n",
    "# S3 Bucket where the data is stored\n",
    "BUCKET_NAME = \"aws-sagemaker-blogpost\"\n",
    "BUCKET = f's3://{BUCKET_NAME}'\n",
    "\n",
    "# Raw data paths\n",
    "RAW_DATA_FOLDER = 'data'\n",
    "# Note if you change the line below, you also need to change it in the cell that creates the prepare_data.py\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "RAW_DATA_PATH = os.path.join(BUCKET, RAW_DATA_FOLDER, RAW_DATA_FILE)\n",
    "\n",
    "# Path where the processed objects will be stored\n",
    "now = datetime.now() # get current time to ensure uniqueness of the output folders\n",
    "PROCESSED_DATA_FOLDER = 'processed_' + now.strftime(\"%Y-%m-%d_%H%M_%S%f\")\n",
    "PROCESSED_DATA_PATH = os.path.join(BUCKET, PROCESSED_DATA_FOLDER)\n",
    "\n",
    "# Paths for model train, validation, test split\n",
    "TRAIN_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'train.csv')\n",
    "VALIDATION_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'validation.csv')\n",
    "TEST_DATA_PATH = os.path.join(PROCESSED_DATA_PATH, 'test.csv')\n",
    "\n",
    "# Path to model output data\n",
    "MODEL_OUTPUT = os.path.join(BUCKET, 'output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job name for preprocessing\n",
    "PREPROCESSING_JOB_NAME = 'windTurbinesPreprocessing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN     49416\n",
       "5.0      2415\n",
       "20.0      235\n",
       "3.0       175\n",
       "19.0      123\n",
       "24.0       10\n",
       "22.0        9\n",
       "Name: categories_sk, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(RAW_DATA_PATH)\n",
    "df.loc[:, 'categories_sk'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN    49416\n",
       "1.0     2424\n",
       "0.0      543\n",
       "Name: subtraction, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'subtraction'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data (TODO)\n",
    "\n",
    "The data comes in chunks, but for our purposes, we would like to have one large file with data. We could also extend the code to read only new data and add it to the file, but for simplicity, let's load the whole dataset for processing.\n",
    "\n",
    "Note: Having the data in chunks would allow for online processing of new data when it comes in, e.g., when a turbine reports the next couple of months data.\n",
    "\n",
    "For details on the dataset, see the exploratory data analysis work in Part 1.\n",
    "\n",
    "* TODO: Expand\n",
    "* TODO: Link to EDA.\n",
    "\n",
    "Amazon SageMaker XGBoost can train on data in either a CSV or LibSVM format. Here, we stick to CSV. The requirements on the data formatting are:\n",
    "\n",
    "1. The target variable must be in the first column.\n",
    "2. There cannot be a header row.\n",
    "\n",
    "We first create a preprocessing script that will be used by a built-in SKLearn container to run the preprocessing job of our pipeline. Running the cell below will produce a Python file whose content is the same as the cell's content and named `prepare_data.py`. It will be saved in the notebooks instance path (see left pane of the SageMaker Studio JupyterLab).\n",
    "\n",
    "We then define the container that will read in the data from the S3 bucket and use this script to preprocess it and then write the processed files back to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prepare_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_data.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "def assert_col_of_df(df: pd.DataFrame, col: Union[List[str], str]) -> None:\n",
    "    \"\"\"Helper function to assert that a column `col` is a column of `df`.\n",
    "    \n",
    "    Args:\n",
    "        df: Dataframe.\n",
    "        col: String value to test.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "        \n",
    "    Raises:\n",
    "        ValueError if `col` is not a column of `df`.\n",
    "    \"\"\"\n",
    "    if isinstance(col, str):\n",
    "        col = [col]\n",
    "\n",
    "    for c in col:\n",
    "        try:\n",
    "            assert c in df.columns      \n",
    "        except AssertionError:\n",
    "            raise ValueError(f\"Invalid input value. Column {c} is not a column of df.\")\n",
    "\n",
    "            \n",
    "def get_train_test_split(\n",
    "        df: pd.DataFrame,\n",
    "        n_days_test: int\n",
    "        ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits the input data frame into a training and test set.\n",
    "\n",
    "    Args:\n",
    "        df: Raw input data.\n",
    "        n_days_test: Number of days to consider for the test split. The n_days_test last \n",
    "            days of the input data will be selected for the test split.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: Raw train and test data splits.\n",
    "    \"\"\"\n",
    "    _date_col = 'date'\n",
    "    _measured_at_col = 'measured_at'\n",
    "    \n",
    "    assert_col_of_df(df=df, col=_measured_at_col)\n",
    "    \n",
    "    # Take only the date part of the string, i.e., the first 10 characters\n",
    "    df[_date_col] = df[_measured_at_col].apply(lambda x: x[:10])\n",
    "    # Convert to date object\n",
    "    df[_date_col] = pd.to_datetime(df[_date_col], format='%Y-%m-%d')\n",
    "    \n",
    "    # Get the test dates\n",
    "    min_date = df[_date_col].min()\n",
    "    max_date = df[_date_col].max()\n",
    "    \n",
    "    test_dates = [\n",
    "        datetime.strftime(max_date - timedelta(days=i), '%Y-%m-%d') for i in range(n_days_test)\n",
    "    ]\n",
    "    \n",
    "    df_train = df[~df[_date_col].isin(test_dates)].drop(_date_col, axis=1)\n",
    "    df_test = df[df[_date_col].isin(test_dates)].drop(_date_col, axis=1)\n",
    "    \n",
    "    logger.info(f\"Train set ranges from {min_date} until {min(test_dates)} (not included).\")\n",
    "    logger.info(f\"Test set ranges from {min(test_dates)} until {max(test_dates)}.\")\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def fill_nulls(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Fills nulls in column `col` of dataframe `df`.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col: Column of `df` with nulls filled with 0.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe with nulls filled.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col)\n",
    "        \n",
    "    df.loc[:, col] = (\n",
    "        df\n",
    "        .loc[:, col]\n",
    "        .fillna(0)\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Filled nulls in column {col} with 0.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_power(df: pd.DataFrame, col_power: str, min_power: float) -> pd.DataFrame:\n",
    "    \"\"\"Filters the `df` on the power column `col_power`.\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        df: Raw input dataframe.\n",
    "        col_power: Column of `df` with the power production.\n",
    "        min_power: Minimum values of power production considered. Rows with smaller\n",
    "            values are filtered out.\n",
    "        \n",
    "    Returns:\n",
    "        Dataframe filtered on `min_power`.\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=col_power)\n",
    "    \n",
    "    filter_power = df[col_power] > min_power\n",
    "    \n",
    "    rowcount_before = df.shape[0]\n",
    "    df = (\n",
    "        df\n",
    "        .loc[filter_power]\n",
    "    )\n",
    "    rowcount_after = df.shape[0]\n",
    "    logger.info(f\"Removed {rowcount_before-rowcount_after} rows which had power below {min_power}.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def split_features_target(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    target: str\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Function to split the dataframe into input features and labels.\n",
    "    \n",
    "    Args:\n",
    "        df: Raw input data.\n",
    "        features: List of the features to be included in the transformation.\n",
    "        target: Target column.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame]: Transformed dataframes (input features and labels).\n",
    "    \"\"\"\n",
    "    assert_col_of_df(df=df, col=(features + [target]))\n",
    "\n",
    "    y = df.loc[:, target]\n",
    "    x = df.loc[:, features]\n",
    "    logger.info(f\"Split dataframe into features with shape {x.shape} and target with shape {y.shape}.\")\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def wrap_transform_data(\n",
    "    df: pd.DataFrame,\n",
    "    col_power: str,\n",
    "    min_power: float,\n",
    "    col_errors: str,\n",
    "    errors_to_classify: List[int],\n",
    "    features: List[str],\n",
    "    target: str\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Wrapper for transforming the data for the model\n",
    "    \n",
    "    Processing is applied in the following steps:\n",
    "        1. Filtering out low power values\n",
    "        2. Fill nulls in all of the feature columns\n",
    "        3. Split dataset into features and target (x, y)\n",
    "        \n",
    "    Args:\n",
    "        df: Input dataframe.\n",
    "        col_power: Column of `df` with the power production.\n",
    "        min_power: Minimum values of power production considered. Rows with smaller\n",
    "            values are filtered out.\n",
    "        features: List of the features to be included in the transformation.\n",
    "        target: Target column.\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    # 1. Filter out low power\n",
    "    df = filter_power(df=df, col_power=col_power, min_power=min_power)\n",
    "\n",
    "    # 2. Fill nulls in all of the feature columns\n",
    "    for feat in features:\n",
    "        df = fill_nulls(df=df, col=feat)\n",
    "\n",
    "    # 3. Split dataset into features and target\n",
    "    x, y = split_features_target(df=df, features=features, target=target) \n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# ----- CONSTANTS ----- #\n",
    "# Columns of df\n",
    "# Error column <> target\n",
    "COL_ERRORS = 'subtraction'\n",
    "# Power produced column (used for filtering out small values)\n",
    "COL_POWER = 'power'\n",
    "# Features to consider for the model\n",
    "FEATURES = ['wind_speed', 'power', 'nacelle_direction', 'wind_direction',\n",
    "            'rotor_speed', 'generator_speed', 'temp_environment',\n",
    "            'temp_hydraulic_oil', 'temp_gear_bearing', 'cosphi',\n",
    "            'blade_angle_avg', 'hydraulic_pressure']\n",
    "# Power values to filter out\n",
    "MIN_POWER = 0.05\n",
    "# Filname of the raw data file\n",
    "RAW_DATA_FILE = 'wind_turbines.csv'\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    logger.info(f'Preprocessing job started.')\n",
    "    # Parse the SDK arguments that are passed when creating the SKlearn container\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_test_days\", type=int, default=10)\n",
    "    parser.add_argument(\"--n_val_days\", type=int, default=10)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.info(f\"Received arguments {args}.\")\n",
    "\n",
    "    # Read in data locally in the container\n",
    "    input_data_path = os.path.join(\"/opt/ml/processing/input\", RAW_DATA_FILE)\n",
    "    logger.info(f\"Reading input data from {input_data_path}\")\n",
    "    # Read raw input data\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    logger.info(f\"Shape of data is: {df.shape}\")\n",
    "\n",
    "    # ---- Preprocess the data set ----\n",
    "    logger.info(\"Split data into training+validation and test set.\")\n",
    "    df_train_valid, df_test = get_train_test_split(df=df, n_days_test=args.n_test_days) \n",
    "\n",
    "    logger.info(\"Split training+validation into training and validation set.\")\n",
    "    df_train, df_val = get_train_test_split(df=df_train_valid, n_days_test=args.n_val_days) \n",
    "\n",
    "    logger.info(\"Transforming training data.\")\n",
    "    x_train, y_train = wrap_transform_data(\n",
    "        df=df_train,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        col_errors=COL_ERRORS,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Transforming validation data.\")\n",
    "    x_val, y_val = wrap_transform_data(\n",
    "        df=df_val,\n",
    "        col_power=COL_POWER,\n",
    "        min_power=MIN_POWER,\n",
    "        col_errors=COL_ERRORS,\n",
    "        features=FEATURES,\n",
    "        target=COL_ERRORS\n",
    "    )\n",
    "\n",
    "    # Create local output directories. These directories live on the container that is spun up.\n",
    "    try:\n",
    "        os.makedirs(\"/opt/ml/processing/train\")\n",
    "        os.makedirs(\"/opt/ml/processing/validation\")\n",
    "        os.makedirs(\"/opt/ml/processing/test\")\n",
    "        print(\"Successfully created directories\")\n",
    "    except Exception as e:\n",
    "        # if the Processing call already creates these directories (or directory otherwise cannot be created)\n",
    "        logger.debug(e)\n",
    "        logger.debug(\"Could Not Make Directories.\")\n",
    "        pass\n",
    "\n",
    "    # Save data locally on the container that is spun up.\n",
    "    try:\n",
    "        pd.DataFrame(x_train).to_csv(\"/opt/ml/processing/train/x_train.csv\", header=True, index=False)\n",
    "        pd.DataFrame(y_train).to_csv(\"/opt/ml/processing/train/y_train.csv\", header=True, index=False)\n",
    "        pd.DataFrame(x_val).to_csv(\"/opt/ml/processing/validation/x_val.csv\", header=True, index=False)\n",
    "        pd.DataFrame(y_val).to_csv(\"/opt/ml/processing/validation/y_val.csv\", header=True, index=False)\n",
    "        pd.DataFrame(df_test).to_csv(\"/opt/ml/processing/test/test.csv\", header=True, index=False)\n",
    "        logger.info(\"Files Successfully Written Locally\")\n",
    "    except Exception as e:\n",
    "        logger.debug(\"Could Not Write the Files\")\n",
    "        logger.debug(e)\n",
    "        pass\n",
    "\n",
    "    logger.info(\"Finished running processing job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one reads the script carefully one can see that the training, validation, and test datasets are stored in the file path `/opt/ml/processing/...`. This is not the path in the S3 bucket, but refers to the local path on the container that is spun up.\n",
    "\n",
    "We specified that in the beginning when setting up path variables (e.g., `TEST_DATA_PATH`)! This will become useful later.\n",
    "The first line of the cell's output will indicate the job's name. Go back to the AWS SageMaker Console and check the processing jobs. In the author's case - eu-central-1 AZ - the link is https://eu-central-1.console.aws.amazon.com/sagemaker/home?region=eu-central-1#/processing-jobs.\n",
    "\n",
    "From there, one can leverage CloudWatch to check the logs for the job or make use of other monitoring tools - the full power of AWS can be leveraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  windTurbinesPreprocessing-2022-06-02-15-43-12-114\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://aws-sagemaker-blogpost/data/wind_turbines.csv', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-451811961115/windTurbinesPreprocessing-2022-06-02-15-43-12-114/input/code/prepare_data.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aws-sagemaker-blogpost/processed_2022-06-02_1543_09970865', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-2', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aws-sagemaker-blogpost/processed_2022-06-02_1543_09970865', 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'output-3', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://aws-sagemaker-blogpost/processed_2022-06-02_1543_09970865', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................\u001b[34mPreprocessing job started.\u001b[0m\n",
      "\u001b[34mReceived arguments Namespace(n_test_days=20, n_val_days=30).\u001b[0m\n",
      "\u001b[34mReading input data from /opt/ml/processing/input/wind_turbines.csv\u001b[0m\n",
      "\u001b[34mShape of data is: (52383, 16)\u001b[0m\n",
      "\u001b[34mSplit data into training+validation and test set.\u001b[0m\n",
      "\u001b[34mTrain set ranges from 2020-01-01 00:00:00 until 2020-03-12 (not included).\u001b[0m\n",
      "\u001b[34mTest set ranges from 2020-03-12 until 2020-03-31.\u001b[0m\n",
      "\u001b[34mSplit training+validation into training and validation set.\u001b[0m\n",
      "\u001b[34mTrain set ranges from 2020-01-01 00:00:00 until 2020-02-11 (not included).\u001b[0m\n",
      "\u001b[34mTest set ranges from 2020-02-11 until 2020-03-11.\u001b[0m\n",
      "\u001b[34mTransforming training data.\u001b[0m\n",
      "\u001b[34mRemoved 3174 rows which had power below 0.05.\u001b[0m\n",
      "\u001b[34mTransformed error types in column categories_sk.\u001b[0m\n",
      "\u001b[34mFilled nulls in column wind_speed with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column power with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column nacelle_direction with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column wind_direction with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column rotor_speed with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column generator_speed with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column temp_environment with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column temp_hydraulic_oil with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column temp_gear_bearing with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column cosphi with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column blade_angle_avg with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column hydraulic_pressure with 0.\u001b[0m\n",
      "\u001b[34mSplit dataframe into features with shape (20409, 12) and target with shape (20409,).\u001b[0m\n",
      "\u001b[34mTransforming validation data.\u001b[0m\n",
      "\u001b[34mRemoved 856 rows which had power below 0.05.\u001b[0m\n",
      "\u001b[34mTransformed error types in column categories_sk.\u001b[0m\n",
      "\u001b[34mFilled nulls in column wind_speed with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column power with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column nacelle_direction with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column wind_direction with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column rotor_speed with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column generator_speed with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column temp_environment with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column temp_hydraulic_oil with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column temp_gear_bearing with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column cosphi with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column blade_angle_avg with 0.\u001b[0m\n",
      "\u001b[34mFilled nulls in column hydraulic_pressure with 0.\u001b[0m\n",
      "\u001b[34mSplit dataframe into features with shape (16424, 12) and target with shape (16424,).\u001b[0m\n",
      "\u001b[34mFiles Successfully Written Locally\u001b[0m\n",
      "\u001b[34mFinished running processing job\u001b[0m\n",
      "\n",
      "CPU times: user 817 ms, sys: 26.5 ms, total: 843 ms\n",
      "Wall time: 4min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_job_name = PREPROCESSING_JOB_NAME\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name=base_job_name,\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=ROLE,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code=\"prepare_data.py\",\n",
    "    inputs=[ProcessingInput(source=RAW_DATA_PATH, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(\n",
    "            destination=PROCESSED_DATA_PATH,\n",
    "            source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    arguments=[\"--n_test_days\", \"20\",\n",
    "              \"--n_val_days\", \"30\"],\n",
    ")\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "For this classification task, we use a bult-in XGBoost container provided by SageMaker. With `sagemaker.image_uri` we specify the training container image URI. In our case, the SageMaker XGBoost training container URI is specified using `sagemaker.image_uris.retrieve`.\n",
    "\n",
    "You can find information on pre-built containers for example here\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html or by exploring the SageMaker example notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker container: 492215442770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3 in region eu-central-1\n"
     ]
    }
   ],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region_name,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "print(f'Using SageMaker container: {container} in region {region_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the SageMaker Estimator with resource configurations and hyperparameters to train a linear regressor on an c4.4xlarge instance. For different types of EC2 instances available from SageMaker, see https://aws.amazon.com/ec2/instance-explorer/?ec2-instances-cards.sort-by=item.additionalFields.category-order&ec2-instances-cards.sort-order=asc&awsf.ec2-instances-filter-category=*all&awsf.ec2-instances-filter-processors=*all&awsf.ec2-instances-filter-accelerators=*all&awsf.ec2-instances-filter-capabilities=*all\n",
    "\n",
    "Specifying the `rules` with `create_xgboost_report()` creates an XGBoost report that provides insights into the training progress and results (see more information here: https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-training-xgboost-report.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://aws-sagemaker-blogpost/processed_2022-06-02_1543_09970865/train.csv'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "xgboost_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.4xlarge',\n",
    "    output_path=MODEL_OUTPUT,\n",
    "    sagemaker_session=sess,\n",
    "    rules=[Rule.sagemaker(rule_configs.create_xgboost_report())]\n",
    ")\n",
    "\n",
    "# set the model's hyperparameters\n",
    "xgboost_model.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "    silent=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories_sk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   categories_sk\n",
       "0              0\n",
       "1              0\n",
       "2              0\n",
       "3              0\n",
       "4              0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = 's3://aws-sagemaker-blogpost/processed_2022-06-02_1543_09970865/x_train.csv'\n",
    "y_train = 's3://aws-sagemaker-blogpost/processed_2022-06-02_1543_09970865/y_train.csv'\n",
    "x_train = pd.read_csv(x_train)\n",
    "y_train = pd.read_csv(y_train)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories_sk\n",
       "0                19443\n",
       "5                  959\n",
       "3                    4\n",
       "9                    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'subtraction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subtraction'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-1011933cd384>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subtraction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subtraction'"
     ]
    }
   ],
   "source": [
    "x_train['subtraction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
